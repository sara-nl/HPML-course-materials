{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import all packages we need and set the correct device. If a CUDA compatible GPU is found, it will be used. If not, everything will be done on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib notebook\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling the data\n",
    "\n",
    "We make two classes for processing our data. The first class, TextEncoder, will be used to encode and decode text data, since we cannot use strings as input directly. All unique words will be found and mapped to an integer. Apart from the unique words we extract from the data, TextEncoder will also have an unknown token: [UNK]. This token will be used if, during inference, we encounter a word that is not part of our vocabulary.\n",
    "\n",
    "Our second class, TextData, will handle the reading and sampling of our input data. It will use and instance of the TextEncoder class to encode our data. TextData also lets us sample sequences of text. In order to do this, TextData needs an implementation of the \\_\\_getitem\\_\\_ method, which will tell it how to handle indices. We also need a \\_\\_len\\_\\_ method so that our TextData class can work with pytorch's DataLoader, but more on that later.\n",
    "\n",
    "All you really need to understand for now though, is that these two classes read data from a txt file and encode it so we can use it for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder():\n",
    "    def __init__(self, file_path):\n",
    "        self.vocab = set()\n",
    "        self.vocab_size = 0\n",
    "        self.encoder = dict()\n",
    "        self.decoder = dict()\n",
    "\n",
    "        self._extract_vocab(file_path)\n",
    "        self._make_encoder_decoder()\n",
    "    \n",
    "    def _extract_vocab(self, file_path):\n",
    "        with open(file_path, \"r\") as file:\n",
    "            text = file.read()\n",
    "            vocab = text.split()\n",
    "            # Tokenizing the text into words and cleaning each word\n",
    "            vocab = [re.sub('[^A-Za-z0-9]+', '', word.lower()) for word in vocab if word != \"\"]\n",
    "            # Creating a set to keep unique words\n",
    "            vocab = set(vocab)\n",
    "        # Storing vocabulary and its size (+1 for [UNK] token)\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab) + 1 # add one for unknown word token\n",
    "\n",
    "    def _make_encoder_decoder(self):\n",
    "        word_ids = range(1, self.vocab_size) # reserve 0 for unknown words\n",
    "        self.encoder = dict(zip(self.vocab, word_ids))\n",
    "        self.decoder = dict(zip(word_ids, self.vocab))\n",
    "\n",
    "        # add unknown token and id\n",
    "        self.encoder[\"[UNK]\"] = 0\n",
    "        self.decoder[0] = \"[UNK]\"\n",
    "\n",
    "class TextData(Dataset):\n",
    "    def __init__(self, file_path, text_encoder, seq_len):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.text_encoder = text_encoder\n",
    "        self.text = self._read_text(file_path)\n",
    "        self.encoded_text = self.encode_text(self.text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_text) - self.seq_len - 2\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            \"sequence\": self.encoded_text[index : index + self.seq_len],\n",
    "            \"next_tokens\": self.encoded_text[index + 1 : index + self.seq_len + 1],\n",
    "        }\n",
    "\n",
    "    def _read_text(self, file_path):\n",
    "        with open(file_path, \"r\") as file:\n",
    "            return file.read()\n",
    "    \n",
    "    def encode_text(self, text):\n",
    "        all_words = text.split()\n",
    "        all_words = [word.lower() for word in all_words if word != \"\"]\n",
    "        encoded_words = [self.text_encoder.encoder[re.sub('[^A-Za-z0-9]+', '', word.lower())] if word in self.text_encoder.vocab else self.text_encoder.encoder[\"[UNK]\"] for word in all_words]\n",
    "\n",
    "        return np.asarray(encoded_words)\n",
    "\n",
    "    def decode_text(self, tokens):\n",
    "        sentence = []\n",
    "        for token in tokens:\n",
    "            #if token==0:\n",
    "            #    continue\n",
    "            sentence.append(self.text_encoder.decoder[token])\n",
    "\n",
    "        return \" \".join(sentence)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Once we can process our data, we need a model. In this notebook we will train a GPT (Generative Pretrained Transformer) model with pytorch, which we define as a class here. We choose which layers we want our model to have and define the forward pass, as well as a function that generates text.\n",
    "\n",
    "The model consists of three parts: \n",
    "\n",
    "<ol>\n",
    "    <li> Embedding layer. This part learns vector representations for each word in the vocabulary. Additionally, it incorporates positional embeddings to retain sequential information since transformers do not inherently comprehend order.\n",
    "    <li> Transformer Blocks: Constituting the core of the GPT model, these blocks contain the self-attention mechanism and multi-layer perceptrons (MLP). The self-attention mechanism enables the model to focus on different words for a given input, maintaining a contextual relationship between words in a sequence. Moreover, each block processes the input data in parallel (unlike RNNs or LSTMs), efficiently handling dependencies between words or sub-words.\n",
    "    <li> Linear classifier. This is an extra layer after the Transformers blocks that makes the prediction. It outputs the logits for each word in the vocabulary, which can then be transformed into probabilities using a softmax function. The word with the highest probability can be selected as the next word in the sequence during text generation\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Full definition of a GPT Language Model, all of it in this single file.\n",
    "References:\n",
    "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
    "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "2) huggingface/transformers PyTorch implementation:\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "3) Taken and modified from the minimal GPT implementation: https://github.com/karpathy/nanoGPT/blob/master/model.py  \n",
    "\"\"\"\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        \n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                    .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "       \n",
    "        # manual implementation of attention\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y, att\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_attn, attn = self.attn(self.ln_1(x))\n",
    "        x = x + x_attn\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x, attn\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.1\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x, attn = block(x)\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # return final logits and last attention matrix of shape (B, NH, T, T)\n",
    "        return logits, attn\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, sample=True, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            \n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, attn = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            if sample:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                idx_next = torch.argmax(probs, dim=1).unsqueeze(0)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions for model training and evaluation\n",
    "\n",
    "The most important function we're creating here is train\\_loop, as this is how we train our model. The generate function is used to generate text. It is called at the end of every epoch, so we can see the model improve. The last function here is print\\_model\\_size, which we simply add to give you a sense of how big such a model is. If you do play around with the hyperparameters above, you can see how this influences not only the performance, but also the size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, dataloader, criterion, optimizer, EPOCHS, PROMPT, LOGGING_INTERVAL=10):\n",
    "    for epoch in range(EPOCHS):\n",
    "        losses = []\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            model.train()\n",
    "\n",
    "            input_sequence = batch[\"sequence\"].to(device)\n",
    "            next_tokens = batch[\"next_tokens\"].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(input_sequence)\n",
    "\n",
    "            loss = criterion(outputs.permute(0,2,1), next_tokens)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "                \n",
    "        if (epoch + 1) % LOGGING_INTERVAL == 0:    \n",
    "            print(f\"[{epoch + 1}] train loss: {np.mean(losses):.3f}\")\n",
    "            sentence, _ = generate_sentence(model, dataloader, PROMPT, 32, sample=False)\n",
    "            print(\"generated sentence: \", sentence)\n",
    "                \n",
    "    print(\"Finished training.\")\n",
    "\n",
    "def generate_sentence(model, dataloader, sentence, gen_len, temperature=0.7, sample=False):\n",
    "    \"\"\"\n",
    "    Generate a text sequence using the trained model based on a provided starting sentence.\n",
    "    \n",
    "    Parameters:\n",
    "        model (torch.nn.Module): The trained GPT model for text generation.\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader providing access to encoding/decoding utilities.\n",
    "        sentence (str): The initial seed text for generation.\n",
    "        gen_len (int): The number of additional tokens to generate.\n",
    "        temperature (float, optional): Controls the randomness/creativity of output; defaults to 0.7.\n",
    "        sample (bool, optional): If True, samples from the output distribution; defaults to False.\n",
    "    \n",
    "    Returns:\n",
    "        sentence (str): The generated text sequence.\n",
    "        attn (torch.Tensor): Attention weights from the model's last layer.\n",
    "    \"\"\"\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # tokenize your prompt text, and send it to device\n",
    "    x = torch.tensor(np.array([dataloader.dataset.encode_text(sentence)])).to(device)\n",
    "\n",
    "    # Generate new token ids using the model\n",
    "    generated_tokens, attn = model.generate(x, gen_len, sample=sample, temperature=temperature)\n",
    "    \n",
    "    # Convert the tensor of token IDs to a list and decode them into normal text\n",
    "    generated_tokens = generated_tokens.view(-1).cpu().numpy().tolist()\n",
    "    sentence = dataloader.dataset.decode_text(generated_tokens)\n",
    "\n",
    "    return sentence, attn\n",
    "    \n",
    "def print_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement()*param.element_size()\n",
    "\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement()*buffer.element_size()\n",
    "\n",
    "    size = (param_size + buffer_size) / 1024**2\n",
    "    print(\"model number of params: \", sum([np.prod(p.size()) for p in model.parameters()]))\n",
    "    print(\"model size: {:.3f}MB\".format(size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your GPT!\n",
    "\n",
    "Here we call the individual parts we defined above and actually do the training.\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Here we set some hyperparameters that will define how the model is trained. You can leave them as is or play with them and see what happens. The configurable training parameters are:\n",
    "<ul>\n",
    "    <li> Batch size. \n",
    "    <li> Epochs. More epochs allow the model more opportunities to learn from the data, but beware of overfitting!\n",
    "    <li> Learning rate.\n",
    "    <li> Sequence length. This determines the length of the sequences that the model will learn from. It's pivotal to balance: shorter sequences might lack context, while longer ones could challenge memory limits and computational efficiency.\n",
    "</ul>\n",
    "\n",
    "### Customizable Data and Prompt\n",
    "\n",
    "You can also customize the data feed to the model and the prompt for testing the accuracy of the model.\n",
    "You can change the DATA\\_FILE\\_PATH to any of the txt files in the Data folder, that way you can choose which data you want to train on. You can also type the PROMPT that is used to generate a sentence each epoch. Keep in mind the model will only recognize words it has seen in the txt file you train on!\n",
    "\n",
    "We have the following txt files available:\n",
    "\n",
    "<ul>\n",
    "    <li> alice_in_wonderland.txt\n",
    "    <li> dummy_text.txt\n",
    "    <li> frankenstein.txt\n",
    "    <li> romeo_and_juliet.txt\n",
    "    <li> tolkien.txt\n",
    "</ul>\n",
    "\n",
    "### Some considerations:\n",
    "<ul>\n",
    "    <li> Vocabulary Awareness: The model recognizes and generates words based on its training data. Words or styles not encountered during training might be handled less adeptly.\n",
    "    <li> Hyperparameter Impact: Feel free to experiment with the hyperparameters, observing how alterations influence training dynamics, model performance, and generated text quality.\n",
    "    <li> Data Diversity: Different text files will immerse the model in varied linguistic environments. Observe how training on 'frankenstein.txt' might differ from 'alice_in_wonderland.txt' in influencing the model's text generation style!\n",
    "    <li> Prompt Creativity: Utilize prompts to navigate the model's text generation. Observe how different prompts or seeds initiate varied continuations by the model.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 1000\n",
    "LEARNING_RATE = 1e-3\n",
    "SEQUENCE_LEN = 32\n",
    "LOGGING_INTERVAL = 10\n",
    "\n",
    "DATA_FILE_PATH = \"./Data/dummy_text.txt\"\n",
    "PROMPT = \"contrary\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now let's train..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences in the dataset: 30\n",
      "Vocabulary size: 256\n",
      "model number of params:  11200\n",
      "model size: 0.051MB\n",
      "[10] train loss: 4.595\n",
      "generated sentence:  lorem [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "[20] train loss: 4.579\n",
      "generated sentence:  lorem [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "[30] train loss: 4.536\n",
      "generated sentence:  lorem [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "[40] train loss: 4.060\n",
      "generated sentence:  lorem [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "[50] train loss: 3.592\n",
      "generated sentence:  lorem [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "[60] train loss: 3.093\n",
      "generated sentence:  lorem [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] [UNK] and [UNK] and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] [UNK]\n",
      "[70] train loss: 2.641\n",
      "generated sentence:  lorem ipsum [UNK] it it it it it it it it [UNK] and [UNK] and [UNK] [UNK] and [UNK] and [UNK] [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and\n",
      "[80] train loss: 2.352\n",
      "generated sentence:  lorem [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] and [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] and [UNK] [UNK] and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] [UNK] and\n",
      "[90] train loss: 1.996\n",
      "generated sentence:  lorem ipsum [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] and [UNK] [UNK] and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "[100] train loss: 1.748\n",
      "generated sentence:  lorem ipsum is not simply random [UNK] [UNK] [UNK] and [UNK] and [UNK] [UNK] [UNK] and [UNK] and [UNK] by reproduced in a latin professor to and [UNK] [UNK] and [UNK] and [UNK]\n",
      "[110] train loss: 1.555\n",
      "generated sentence:  lorem ipsum [UNK] and going through the first line of lorem ipsum [UNK] [UNK] [UNK] and [UNK] and [UNK] and [UNK] [UNK] and it has a lorem ipsum is not are and [UNK]\n",
      "[120] train loss: 1.365\n",
      "generated sentence:  lorem ipsum [UNK] [UNK] and [UNK] [UNK] [UNK] and [UNK] [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and [UNK]\n",
      "[130] train loss: 1.238\n",
      "generated sentence:  lorem ipsum [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] and [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] [UNK] and [UNK] [UNK] and [UNK] and [UNK] and [UNK] [UNK] and\n",
      "[140] train loss: 1.154\n",
      "generated sentence:  lorem ipsum [UNK] [UNK] and [UNK] [UNK] [UNK] and [UNK] [UNK] and [UNK] and [UNK] and [UNK] finibus bonorum et [UNK] [UNK] [UNK] look roots in a piece from [UNK] [UNK] [UNK] [UNK]\n",
      "[150] train loss: 0.973\n",
      "generated sentence:  lorem ipsum [UNK] [UNK] and [UNK] [UNK] [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and [UNK] finibus bonorum et [UNK] [UNK] look roots in a piece from [UNK] [UNK] [UNK] [UNK]\n",
      "[160] train loss: 0.868\n",
      "generated sentence:  lorem ipsum [UNK] [UNK] and going through the theory of [UNK] and [UNK] and [UNK] and [UNK] finibus bonorum et [UNK] anything embarrassing hidden in the middle of [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "[170] train loss: 0.781\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] the word in classical [UNK] the [UNK] of [UNK] it to make a long established fact that a page\n",
      "[180] train loss: 0.704\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] the word in classical [UNK] the standard chunk of lorem ipsum used since the 1500s is reproduced below for\n",
      "[190] train loss: 0.696\n",
      "generated sentence:  lorem ipsum [UNK] and [UNK] and [UNK] finibus bonorum et [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] content from roots in the middle of [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "[200] train loss: 0.570\n",
      "generated sentence:  lorem ipsum [UNK] [UNK] and a latin professor at [UNK] [UNK] [UNK] [UNK] and [UNK] and [UNK] from [UNK] and [UNK] finibus bonorum et [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "[210] train loss: 0.505\n",
      "generated sentence:  lorem [UNK] [UNK] [UNK] and [UNK] [UNK] finibus bonorum et [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] by [UNK] by [UNK] finibus bonorum et [UNK] by cicero are also reproduced in their exact\n",
      "[220] train loss: 0.438\n",
      "generated sentence:  lorem [UNK] [UNK] and [UNK] and [UNK] finibus bonorum et [UNK] [UNK] [UNK] by cicero are also reproduced in their exact original [UNK] accompanied by english versions from the 1914 translation by [UNK]\n",
      "[230] train loss: 0.386\n",
      "generated sentence:  lorem ipsum is not simply random [UNK] and [UNK] and [UNK] and [UNK] finibus bonorum et [UNK] [UNK] by [UNK] by [UNK] written in 45 [UNK] by injected [UNK] or randomised words which\n",
      "[240] train loss: 0.345\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] and [UNK] of [UNK] finibus bonorum et\n",
      "[250] train loss: 0.319\n",
      "generated sentence:  lorem ipsum [UNK] and going through the theory of [UNK] very popular during the first line of lorem [UNK] from a dictionary of over 200 latin [UNK] combined with a handful of model\n",
      "[260] train loss: 0.297\n",
      "generated sentence:  lorem [UNK] [UNK] and [UNK] or randomised words which [UNK] look even slightly [UNK] if you are going to use a passage of lorem [UNK] you need to be sure there [UNK] anything\n",
      "[270] train loss: 0.243\n",
      "generated sentence:  lorem ipsum [UNK] and a [UNK] from a search for [UNK] will uncover many web sites still in their [UNK] various versions have evolved over the [UNK] sometimes by [UNK] sometimes by [UNK]\n",
      "[280] train loss: 0.218\n",
      "generated sentence:  lorem ipsum [UNK] and a search for [UNK] will uncover many web sites still in their [UNK] the [UNK] the standard dummy text ever since the point of using lorem ipsum is that\n",
      "[290] train loss: 0.202\n",
      "generated sentence:  lorem ipsum [UNK] and a search for [UNK] will uncover many web sites still in their [UNK] various versions have evolved over the [UNK] sometimes by [UNK] sometimes by [UNK] humour and the\n",
      "[300] train loss: 0.187\n",
      "generated sentence:  lorem ipsum [UNK] and a search for [UNK] will uncover many web sites still in their [UNK] various versions have evolved over the [UNK] sometimes by [UNK] sometimes by [UNK] sometimes by [UNK]\n",
      "[310] train loss: 0.179\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] of [UNK] finibus bonorum et [UNK] [UNK]\n",
      "[320] train loss: 0.182\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] and [UNK] from [UNK] finibus bonorum et\n",
      "[330] train loss: 0.170\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] of [UNK] finibus bonorum et [UNK] [UNK]\n",
      "[340] train loss: 0.161\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] of [UNK] finibus bonorum et [UNK] [UNK]\n",
      "[350] train loss: 0.154\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] of [UNK] finibus bonorum et [UNK] [UNK]\n",
      "[360] train loss: 0.148\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] of [UNK] finibus bonorum et [UNK] [UNK]\n",
      "[370] train loss: 0.152\n",
      "generated sentence:  lorem ipsum [UNK] and a [UNK] and [UNK] from [UNK] and [UNK] from [UNK] finibus bonorum et [UNK] by cicero are also reproduced in their exact original [UNK] accompanied by english versions from\n",
      "[380] train loss: 0.145\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] of [UNK] finibus bonorum et [UNK] [UNK]\n",
      "[390] train loss: 0.138\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] of [UNK] finibus bonorum et [UNK] [UNK]\n",
      "[400] train loss: 0.133\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] of [UNK] finibus bonorum et [UNK] [UNK]\n",
      "[410] train loss: 0.142\n",
      "generated sentence:  lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a latin professor at [UNK]\n",
      "[420] train loss: 0.129\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] of [UNK] finibus bonorum et [UNK] [UNK]\n",
      "[430] train loss: 0.134\n",
      "generated sentence:  lorem ipsum [UNK] and [UNK] and [UNK] from [UNK] finibus bonorum et [UNK] by cicero are also reproduced in their exact original [UNK] accompanied by english versions from the 1914 translation by [UNK]\n",
      "[440] train loss: 0.124\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] of [UNK] finibus bonorum et [UNK] [UNK]\n",
      "[450] train loss: 0.118\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] of [UNK] finibus bonorum et [UNK] [UNK]\n",
      "[460] train loss: 0.113\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] of [UNK] finibus bonorum et [UNK] [UNK]\n",
      "[470] train loss: 0.113\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] of [UNK] finibus bonorum et [UNK] [UNK] [UNK] extremes\n",
      "[480] train loss: 0.109\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] of [UNK] finibus bonorum et [UNK] [UNK]\n",
      "[490] train loss: 0.110\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] of [UNK] finibus bonorum et [UNK] [UNK] [UNK] extremes\n",
      "[500] train loss: 0.100\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] of [UNK] finibus bonorum et [UNK] [UNK]\n",
      "[510] train loss: 0.102\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] of [UNK] finibus bonorum et [UNK] [UNK]\n",
      "[520] train loss: 0.098\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] of [UNK] finibus bonorum et [UNK] [UNK]\n",
      "[530] train loss: 0.096\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] of [UNK] finibus bonorum et [UNK] [UNK]\n",
      "[540] train loss: 0.096\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] of [UNK] finibus bonorum et [UNK] extremes\n",
      "[550] train loss: 0.095\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] of [UNK] finibus bonorum et [UNK] [UNK]\n",
      "[560] train loss: 0.138\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] and [UNK] of [UNK] finibus bonorum et\n",
      "[570] train loss: 0.086\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] of [UNK] finibus bonorum et [UNK] [UNK]\n",
      "[580] train loss: 0.103\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] of [UNK] finibus bonorum et [UNK] extremes\n",
      "[590] train loss: 0.089\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] of [UNK] finibus bonorum et [UNK] extremes\n",
      "[600] train loss: 0.090\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] of [UNK] finibus bonorum et [UNK] extremes\n",
      "[610] train loss: 0.092\n",
      "generated sentence:  lorem ipsum [UNK] and more recently with desktop publishing software like aldus pagemaker including versions of lorem [UNK] why do we use [UNK] it is a long established fact that a reader will\n",
      "[620] train loss: 0.107\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] of [UNK] finibus bonorum et [UNK] extremes\n",
      "[630] train loss: 0.087\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] of [UNK] finibus bonorum et [UNK] extremes\n",
      "[640] train loss: 0.082\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] of [UNK] finibus bonorum et [UNK] extremes\n",
      "[650] train loss: 0.079\n",
      "generated sentence:  lorem ipsum [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] of [UNK] finibus bonorum et [UNK] extremes\n",
      "[660] train loss: 0.082\n",
      "generated sentence:  lorem ipsum [UNK] and more recently with desktop publishing software like aldus pagemaker including versions of lorem [UNK] why do we use [UNK] it is a long established fact that a reader will\n",
      "[670] train loss: 0.079\n",
      "generated sentence:  lorem ipsum [UNK] and more recently with desktop publishing software like aldus pagemaker including versions of lorem [UNK] why do we use [UNK] it is a long established fact that a reader will\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     23\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), LEARNING_RATE)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROMPT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLOGGING_INTERVAL\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, dataloader, criterion, optimizer, EPOCHS, PROMPT, LOGGING_INTERVAL)\u001b[0m\n\u001b[1;32m      8\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 11\u001b[0m outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m), next_tokens)\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 136\u001b[0m, in \u001b[0;36mGPT.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m    134\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mdrop(tok_emb \u001b[38;5;241m+\u001b[39m pos_emb)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mh:\n\u001b[0;32m--> 136\u001b[0m     x, attn \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mln_f(x)\n\u001b[1;32m    140\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\n",
      "File \u001b[0;32m/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 92\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     90\u001b[0m x_attn, attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(x))\n\u001b[1;32m     91\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m x_attn\n\u001b[0;32m---> 92\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x, attn\n",
      "File \u001b[0;32m/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 75\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     74\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_fc(x)\n\u001b[0;32m---> 75\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(x)\n\u001b[1;32m     77\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[0;32m/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/sw/arch/RHEL8/EB_production/2023/software/PyTorch/2.1.2-foss-2023a-CUDA-12.1.1/lib/python3.11/site-packages/torch/nn/modules/module.py:1521\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1521\u001b[0m     forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tracing_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1522\u001b[0m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text_encoder = TextEncoder(DATA_FILE_PATH)\n",
    "dataset = TextData(DATA_FILE_PATH, text_encoder, SEQUENCE_LEN)\n",
    "dataloader = DataLoader(dataset, BATCH_SIZE)\n",
    "vocab_size = text_encoder.vocab_size\n",
    "print(f\"Number of sequences in the dataset: {len(dataloader)}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Model Hyperparams\n",
    "model_args = dict(n_layer=2, \n",
    "                  n_head=2, \n",
    "                  n_embd=16, \n",
    "                  block_size=SEQUENCE_LEN, # sequence length\n",
    "                  bias=True, \n",
    "                  vocab_size=vocab_size, \n",
    "                  dropout=0.0) \n",
    "\n",
    "gpt_conf = GPTConfig(**model_args)\n",
    "model = GPT(gpt_conf).to(device)\n",
    "\n",
    "print_model_size(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), LEARNING_RATE)\n",
    "\n",
    "train_loop(model, dataloader, criterion, optimizer, EPOCHS, PROMPT, LOGGING_INTERVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a sentence and visualizing attention\n",
    "\n",
    "Now that we trained our model, we can play with it and generate sentences! (Again, keep in mind the model will only recognize words it has seen in the txt file you train on!). We can also visualize the attention.\n",
    "\n",
    "### What is Attention?\n",
    "In the context of transformer models like GPT, attention mechanisms are used to weigh the importance of different input tokens when predicting an output token. \n",
    "\n",
    "Visualizing attention weights can provide insights into the model's decision-making process. By exploring which tokens the model is focusing on when making a prediction, we can gain a better understanding of its learned patterns and potential areas of improvement.\n",
    "\n",
    "### How are we Visualizing Attention?\n",
    "In the following code, we extract the attention weights from our model and visualize them using a heatmap. The x-axis of the heatmap represents input tokens, while the color intensity represents the attention weight – darker colors indicate higher attention.\n",
    "\n",
    "<ul>\n",
    "    <li> Token Selection. We select a token of interest in a sentence and inspect how much attention it pays to other tokens when being predicted.\n",
    "    <li> Head Selection. Transformers contain multiple attention heads, each learning different patterns of attention. We select one of these heads for visualization.\n",
    "    <li> Heatmap. The heatmap represents attention weights from one token (e.g., \"gandalf\") towards all preceding tokens in the sentence.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lorem', 'ipsum', '[UNK]', 'and', 'going', 'through', 'the', 'cites', 'of', 'the', 'word', 'in', 'classical', '[UNK]', 'discovered', 'the', 'undoubtable']\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"Lorem\"\n",
    "sentence, attn = generate_sentence(model, dataloader, PROMPT, 16, temperature=0.8, sample=True)\n",
    "sentence = sentence.split(\" \")\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now, attn_weights is of shape (1, model.n_head, T, T)\n",
    "# We get attention weights for our token of interest for the specified head\n",
    "token_idx = 6 #sentence.index('flying')\n",
    "head = 1\n",
    "\n",
    "attention = attn[0, head, token_idx].cpu().detach().numpy()\n",
    "attention = attention[:token_idx]\n",
    "\n",
    "# Convert the 1D vector to a 2D matrix and transpose for horizontal heatmap\n",
    "attention_matrix = attention.reshape(1, -1)\n",
    "\n",
    "# Now we'll plot the attention weights using a heatmap\n",
    "plt.subplots(figsize=(10, 4))\n",
    "sns.heatmap(attention_matrix, cmap=\"Blues\", annot=True, xticklabels=sentence[:token_idx], cbar_kws={'label': 'Attention Weight'})\n",
    "#sns.heatmap(attention_matrix, cmap=\"Blues\", cbar_kws={'label': 'Attention Weight'}, ax=ax, annot=True)\n",
    "\n",
    "\n",
    "plt.ylabel(f'Attention Weight from word x{sentence[token_idx]}')\n",
    "plt.xlabel('Tokens in Sentence')\n",
    "plt.title(f'Attention Weights of Head {head+1} for Token Index {token_idx}')\n",
    "plt.xticks(rotation=45)  # Rotate token names diagonally for better visibility\n",
    "plt.tight_layout()  # Ensures that everything fits without overlap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "2567d625573dd00306b4578d011ae5968ef0ee87229e2f9d88073f62b5adab8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
