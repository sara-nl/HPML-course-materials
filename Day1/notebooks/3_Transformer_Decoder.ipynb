{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import all packages we need and set the correct device. If a CUDA compatible GPU is found, it will be used. If not, everything will be done on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib notebook\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling the data\n",
    "\n",
    "We make two classes for processing our data. The first class, TextEncoder, will be used to encode and decode text data, since we cannot use strings as input directly. All unique words will be found and mapped to an integer. Apart from the unique words we extract from the data, TextEncoder will also have an unknown token: [UNK]. This token will be used if, during inference, we encounter a word that is not part of our vocabulary.\n",
    "\n",
    "Our second class, TextData, will handle the reading and sampling of our input data. It will use and instance of the TextEncoder class to encode our data. TextData also lets us sample sequences of text. In order to do this, TextData needs an implementation of the \\_\\_getitem\\_\\_ method, which will tell it how to handle indices. We also need a \\_\\_len\\_\\_ method so that our TextData class can work with pytorch's DataLoader, but more on that later.\n",
    "\n",
    "All you really need to understand for now though, is that these two classes read data from a txt file and encode it so we can use it for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder():\n",
    "    def __init__(self, file_path):\n",
    "        self.vocab = set()\n",
    "        self.vocab_size = 0\n",
    "        self.encoder = dict()\n",
    "        self.decoder = dict()\n",
    "\n",
    "        self._extract_vocab(file_path)\n",
    "        self._make_encoder_decoder()\n",
    "    \n",
    "    def _extract_vocab(self, file_path):\n",
    "        with open(file_path, \"r\") as file:\n",
    "            text = file.read()\n",
    "            vocab = text.split()\n",
    "            # Tokenizing the text into words and cleaning each word\n",
    "            vocab = [re.sub('[^A-Za-z0-9]+', '', word.lower()) for word in vocab if word != \"\"]\n",
    "            # Creating a set to keep unique words\n",
    "            vocab = set(vocab)\n",
    "        # Storing vocabulary and its size (+1 for [UNK] token)\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab) + 1 # add one for unknown word token\n",
    "\n",
    "    def _make_encoder_decoder(self):\n",
    "        word_ids = range(1, self.vocab_size) # reserve 0 for unknown words\n",
    "        self.encoder = dict(zip(self.vocab, word_ids))\n",
    "        self.decoder = dict(zip(word_ids, self.vocab))\n",
    "\n",
    "        # add unknown token and id\n",
    "        self.encoder[\"[UNK]\"] = 0\n",
    "        self.decoder[0] = \"[UNK]\"\n",
    "\n",
    "class TextData(Dataset):\n",
    "    def __init__(self, file_path, text_encoder, seq_len):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.text_encoder = text_encoder\n",
    "        self.text = self._read_text(file_path)\n",
    "        self.encoded_text = self.encode_text(self.text)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_text) - self.seq_len - 2\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            \"sequence\": self.encoded_text[index : index + self.seq_len],\n",
    "            \"next_tokens\": self.encoded_text[index + 1 : index + self.seq_len + 1],\n",
    "        }\n",
    "\n",
    "    def _read_text(self, file_path):\n",
    "        with open(file_path, \"r\") as file:\n",
    "            return file.read()\n",
    "    \n",
    "    def encode_text(self, text):\n",
    "        all_words = text.split()\n",
    "        all_words = [word.lower() for word in all_words if word != \"\"]\n",
    "        encoded_words = [self.text_encoder.encoder[re.sub('[^A-Za-z0-9]+', '', word.lower())] if word in self.text_encoder.vocab else self.text_encoder.encoder[\"[UNK]\"] for word in all_words]\n",
    "\n",
    "        return np.asarray(encoded_words)\n",
    "\n",
    "    def decode_text(self, tokens):\n",
    "        sentence = []\n",
    "        for token in tokens:\n",
    "            #if token==0:\n",
    "            #    continue\n",
    "            sentence.append(self.text_encoder.decoder[token])\n",
    "\n",
    "        return \" \".join(sentence)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Once we can process our data, we need a model. In this notebook we will train a GPT (Generative Pretrained Transformer) model with pytorch, which we define as a class here. We choose which layers we want our model to have and define the forward pass, as well as a function that generates text.\n",
    "\n",
    "The model consists of three parts: \n",
    "\n",
    "<ol>\n",
    "    <li> Embedding layer. This part learns vector representations for each word in the vocabulary. Additionally, it incorporates positional embeddings to retain sequential information since transformers do not inherently comprehend order.\n",
    "    <li> Transformer Blocks: Constituting the core of the GPT model, these blocks contain the self-attention mechanism and multi-layer perceptrons (MLP). The self-attention mechanism enables the model to focus on different words for a given input, maintaining a contextual relationship between words in a sequence. Moreover, each block processes the input data in parallel (unlike RNNs or LSTMs), efficiently handling dependencies between words or sub-words.\n",
    "    <li> Linear classifier. This is an extra layer after the Transformers blocks that makes the prediction. It outputs the logits for each word in the vocabulary, which can then be transformed into probabilities using a softmax function. The word with the highest probability can be selected as the next word in the sequence during text generation\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Full definition of a GPT Language Model, all of it in this single file.\n",
    "References:\n",
    "1) the official GPT-2 TensorFlow implementation released by OpenAI:\n",
    "https://github.com/openai/gpt-2/blob/master/src/model.py\n",
    "2) huggingface/transformers PyTorch implementation:\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n",
    "3) Taken and modified from the minimal GPT implementation: https://github.com/karpathy/nanoGPT/blob/master/model.py  \n",
    "\"\"\"\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        \n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                    .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "       \n",
    "        # manual implementation of attention\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y, att\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_attn, attn = self.attn(self.ln_1(x))\n",
    "        x = x + x_attn\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x, attn\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.1\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x, attn = block(x)\n",
    "\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        # return final logits and last attention matrix of shape (B, NH, T, T)\n",
    "        return logits, attn\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, sample=True, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            \n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, attn = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            if sample:\n",
    "                idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            else:\n",
    "                idx_next = torch.argmax(probs, dim=1).unsqueeze(0)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx, attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining functions for model training and evaluation\n",
    "\n",
    "The most important function we're creating here is train\\_loop, as this is how we train our model. The generate function is used to generate text. It is called at the end of every epoch, so we can see the model improve. The last function here is print\\_model\\_size, which we simply add to give you a sense of how big such a model is. If you do play around with the hyperparameters above, you can see how this influences not only the performance, but also the size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, dataloader, criterion, optimizer, EPOCHS, PROMPT, LOGGING_INTERVAL=10):\n",
    "    for epoch in range(EPOCHS):\n",
    "        losses = []\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            model.train()\n",
    "\n",
    "            input_sequence = batch[\"sequence\"].to(device)\n",
    "            next_tokens = batch[\"next_tokens\"].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs, _ = model(input_sequence)\n",
    "\n",
    "            loss = criterion(outputs.permute(0,2,1), next_tokens)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            losses.append(loss.item())\n",
    "                \n",
    "        if (epoch + 1) % LOGGING_INTERVAL == 0:    \n",
    "            print(f\"[{epoch + 1}] train loss: {np.mean(losses):.3f}\")\n",
    "            sentence, _ = generate_sentence(model, dataloader, PROMPT, 32, sample=False)\n",
    "            print(\"generated sentence: \", sentence)\n",
    "                \n",
    "    print(\"Finished training.\")\n",
    "\n",
    "def generate_sentence(model, dataloader, sentence, gen_len, temperature=0.7, sample=False):\n",
    "    \"\"\"\n",
    "    Generate a text sequence using the trained model based on a provided starting sentence.\n",
    "    \n",
    "    Parameters:\n",
    "        model (torch.nn.Module): The trained GPT model for text generation.\n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader providing access to encoding/decoding utilities.\n",
    "        sentence (str): The initial seed text for generation.\n",
    "        gen_len (int): The number of additional tokens to generate.\n",
    "        temperature (float, optional): Controls the randomness/creativity of output; defaults to 0.7.\n",
    "        sample (bool, optional): If True, samples from the output distribution; defaults to False.\n",
    "    \n",
    "    Returns:\n",
    "        sentence (str): The generated text sequence.\n",
    "        attn (torch.Tensor): Attention weights from the model's last layer.\n",
    "    \"\"\"\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # tokenize your prompt text, and send it to device\n",
    "    x = torch.tensor(np.array([dataloader.dataset.encode_text(sentence)])).to(device)\n",
    "\n",
    "    # Generate new token ids using the model\n",
    "    generated_tokens, attn = model.generate(x, gen_len, sample=sample, temperature=temperature)\n",
    "    \n",
    "    # Convert the tensor of token IDs to a list and decode them into normal text\n",
    "    generated_tokens = generated_tokens.view(-1).cpu().numpy().tolist()\n",
    "    sentence = dataloader.dataset.decode_text(generated_tokens)\n",
    "\n",
    "    return sentence, attn\n",
    "    \n",
    "def print_model_size(model):\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement()*param.element_size()\n",
    "\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement()*buffer.element_size()\n",
    "\n",
    "    size = (param_size + buffer_size) / 1024**2\n",
    "    print(\"model number of params: \", sum([np.prod(p.size()) for p in model.parameters()]))\n",
    "    print(\"model size: {:.3f}MB\".format(size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train your GPT!\n",
    "\n",
    "Here we call the individual parts we defined above and actually do the training.\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "Here we set some hyperparameters that will define how the model is trained. You can leave them as is or play with them and see what happens. The configurable training parameters are:\n",
    "<ul>\n",
    "    <li> Batch size. \n",
    "    <li> Epochs. More epochs allow the model more opportunities to learn from the data, but beware of overfitting!\n",
    "    <li> Learning rate.\n",
    "    <li> Sequence length. This determines the length of the sequences that the model will learn from. It's pivotal to balance: shorter sequences might lack context, while longer ones could challenge memory limits and computational efficiency.\n",
    "</ul>\n",
    "\n",
    "### Customizable Data and Prompt\n",
    "\n",
    "You can also customize the data feed to the model and the prompt for testing the accuracy of the model.\n",
    "You can change the DATA\\_FILE\\_PATH to any of the txt files in the Data folder, that way you can choose which data you want to train on. You can also type the PROMPT that is used to generate a sentence each epoch. Keep in mind the model will only recognize words it has seen in the txt file you train on!\n",
    "\n",
    "We have the following txt files available:\n",
    "\n",
    "<ul>\n",
    "    <li> alice_in_wonderland.txt\n",
    "    <li> dummy_text.txt\n",
    "    <li> frankenstein.txt\n",
    "    <li> romeo_and_juliet.txt\n",
    "    <li> tolkien.txt\n",
    "</ul>\n",
    "\n",
    "### Some considerations:\n",
    "<ul>\n",
    "    <li> Vocabulary Awareness: The model recognizes and generates words based on its training data. Words or styles not encountered during training might be handled less adeptly.\n",
    "    <li> Hyperparameter Impact: Feel free to experiment with the hyperparameters, observing how alterations influence training dynamics, model performance, and generated text quality.\n",
    "    <li> Data Diversity: Different text files will immerse the model in varied linguistic environments. Observe how training on 'frankenstein.txt' might differ from 'alice_in_wonderland.txt' in influencing the model's text generation style!\n",
    "    <li> Prompt Creativity: Utilize prompts to navigate the model's text generation. Observe how different prompts or seeds initiate varied continuations by the model.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "EPOCHS = 1000\n",
    "LEARNING_RATE = 1e-3\n",
    "SEQUENCE_LEN = 32\n",
    "LOGGING_INTERVAL = 10\n",
    "\n",
    "DATA_FILE_PATH = \"./Data/dummy_text.txt\"\n",
    "PROMPT = \"contrary\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and now let's train..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sequences in the dataset: 30\n",
      "Vocabulary size: 256\n",
      "model number of params:  11200\n",
      "model size: 0.051MB\n",
      "[10] train loss: 4.592\n",
      "generated sentence:  contrary [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "[20] train loss: 4.576\n",
      "generated sentence:  contrary [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "[30] train loss: 4.460\n",
      "generated sentence:  contrary [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "[40] train loss: 3.820\n",
      "generated sentence:  contrary [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "[50] train loss: 3.319\n",
      "generated sentence:  contrary to the ipsum is the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "[60] train loss: 2.763\n",
      "generated sentence:  contrary to a it has a a a a a it has a it has a it has in the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "[70] train loss: 2.457\n",
      "generated sentence:  contrary [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and [UNK]\n",
      "[80] train loss: 1.990\n",
      "generated sentence:  contrary on the [UNK] [UNK] [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and [UNK] and\n",
      "[90] train loss: 1.680\n",
      "generated sentence:  contrary to use [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] lorem ipsum dolor sit [UNK] and [UNK] and the 1500s is in [UNK] and [UNK] and [UNK] and\n",
      "[100] train loss: 1.429\n",
      "generated sentence:  contrary to the [UNK] lorem ipsum [UNK] lorem ipsum [UNK] lorem ipsum [UNK] and the standard chunk of the standard chunk of the 1500s is reproduced below since going through the word in\n",
      "[110] train loss: 1.211\n",
      "generated sentence:  contrary to use lorem [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] lorem ipsum dolor sit through through going through through through the internet tend to repeat predefined chunks as [UNK] making this the\n",
      "[120] train loss: 1.052\n",
      "generated sentence:  contrary to make a dictionary of lorem ipsum is a lorem ipsum which looks [UNK] look like aldus pagemaker including versions have evolved sheets sheets there many web [UNK] there are also the\n",
      "[130] train loss: 0.858\n",
      "generated sentence:  contrary to be sure by the [UNK] it has roots in a type specimen [UNK] a page in a piece of lorem ipsum [UNK] lorem ipsum [UNK] the [UNK] lorem ipsum is that\n",
      "[140] train loss: 0.672\n",
      "generated sentence:  contrary to be sure by scrambled it over a piece of classical latin literature from 45 [UNK] discovered the lorem ipsum [UNK] lorem ipsum [UNK] lorem ipsum [UNK] lorem ipsum [UNK] and the\n",
      "[150] train loss: 0.570\n",
      "generated sentence:  contrary to be sure there [UNK] it has survived not only five [UNK] but also the leap into electronic [UNK] remaining essentially [UNK] remaining essentially [UNK] it was popularised in their default of\n",
      "[160] train loss: 0.493\n",
      "generated sentence:  contrary to be sure there [UNK] it has survived not only five [UNK] but also the leap into electronic [UNK] it was popularised in the 1960s with the release of lorem ipsum [UNK]\n",
      "[170] train loss: 0.538\n",
      "generated sentence:  contrary to be sure there [UNK] anything embarrassing hidden in the middle of [UNK] all the lorem ipsum [UNK] lorem ipsum [UNK] lorem ipsum [UNK] of [UNK] of [UNK] finibus bonorum et [UNK]\n",
      "[180] train loss: 0.398\n",
      "generated sentence:  contrary to use a passage of lorem [UNK] you need to be sure there [UNK] anything embarrassing hidden in the middle of [UNK] all the lorem ipsum generators on the internet tend to\n",
      "[190] train loss: 0.344\n",
      "generated sentence:  contrary to be sure there [UNK] anything embarrassing hidden in the middle of [UNK] all the lorem ipsum [UNK] lorem ipsum [UNK] lorem ipsum [UNK] of [UNK] of [UNK] finibus bonorum et [UNK]\n",
      "[200] train loss: 0.287\n",
      "generated sentence:  contrary to use a passage of lorem [UNK] you need to be sure there [UNK] anything embarrassing hidden in the middle of [UNK] all the lorem ipsum generators on the internet tend to\n",
      "[210] train loss: 0.279\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum [UNK] of lorem ipsum [UNK] of [UNK] of [UNK] finibus bonorum et [UNK] extremes of good and [UNK] by [UNK] written in 45 [UNK] very popular [UNK]\n",
      "[220] train loss: 0.244\n",
      "generated sentence:  contrary to use a passage of lorem [UNK] you need to be sure there [UNK] anything embarrassing hidden in the middle of [UNK] all the lorem ipsum generators on the internet tend to\n",
      "[230] train loss: 0.218\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum [UNK] lorem ipsum comes from a line in section [UNK] but also in latin [UNK] standard chunk of lorem ipsum used since the 1500s is reproduced below\n",
      "[240] train loss: 0.214\n",
      "generated sentence:  contrary to be sure there [UNK] anything embarrassing hidden in the middle of [UNK] all the lorem ipsum generators on the internet tend to repeat predefined chunks as [UNK] making this the first\n",
      "[250] train loss: 0.207\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[260] train loss: 0.201\n",
      "generated sentence:  contrary to make a type specimen [UNK] it has survived not only five [UNK] but also the leap into electronic [UNK] remaining essentially [UNK] it was popularised in the 1960s with the release\n",
      "[270] train loss: 0.246\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum [UNK] lorem ipsum [UNK] and latin professor at [UNK] but also the an unknown printer took a [UNK] sometimes by [UNK] sometimes by [UNK] sometimes by [UNK]\n",
      "[280] train loss: 0.165\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[290] train loss: 0.150\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[300] train loss: 0.154\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[310] train loss: 0.154\n",
      "generated sentence:  contrary to the [UNK] [UNK] [UNK] content [UNK] and going through the cites of the word in classical [UNK] discovered the undoubtable [UNK] lorem ipsum comes from sections [UNK] and [UNK] and [UNK]\n",
      "[320] train loss: 0.161\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[330] train loss: 0.148\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] look\n",
      "[340] train loss: 0.139\n",
      "generated sentence:  contrary to the [UNK] [UNK] content [UNK] making it look like readable [UNK] many desktop publishing packages and web page editors now use lorem ipsum as their default model [UNK] uses a handful\n",
      "[350] train loss: 0.132\n",
      "generated sentence:  contrary to the first line of lorem [UNK] ipsum dolor sit [UNK] comes from a line in section [UNK] the standard chunk of lorem ipsum used since the 1500s is reproduced below for\n",
      "[360] train loss: 0.129\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[370] train loss: 0.134\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] look\n",
      "[380] train loss: 0.130\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[390] train loss: 0.120\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[400] train loss: 0.116\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[410] train loss: 0.131\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[420] train loss: 0.107\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[430] train loss: 0.103\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[440] train loss: 0.103\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] look\n",
      "[450] train loss: 0.110\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[460] train loss: 0.096\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[470] train loss: 0.090\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[480] train loss: 0.115\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[490] train loss: 0.077\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[500] train loss: 0.076\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[510] train loss: 0.077\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[520] train loss: 0.097\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] look\n",
      "[530] train loss: 0.117\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[540] train loss: 0.089\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[550] train loss: 0.116\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[560] train loss: 0.075\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[570] train loss: 0.076\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[580] train loss: 0.081\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[590] train loss: 0.098\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[600] train loss: 0.091\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] by cicero are also reproduced below for [UNK] [UNK] extremes of good and [UNK] [UNK] [UNK] [UNK] [UNK] from [UNK] finibus bonorum\n",
      "[610] train loss: 0.069\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[620] train loss: 0.069\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[630] train loss: 0.069\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[640] train loss: 0.068\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[650] train loss: 0.068\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[660] train loss: 0.068\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[670] train loss: 0.126\n",
      "generated sentence:  contrary to popular during the [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard\n",
      "[680] train loss: 0.106\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[690] train loss: 0.105\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[700] train loss: 0.089\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[710] train loss: 0.083\n",
      "generated sentence:  contrary to popular [UNK] comes from a lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK]\n",
      "[720] train loss: 0.065\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[730] train loss: 0.065\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[740] train loss: 0.065\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[750] train loss: 0.065\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[760] train loss: 0.065\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[770] train loss: 0.065\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[780] train loss: 0.065\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[790] train loss: 0.065\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[800] train loss: 0.065\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[810] train loss: 0.065\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[820] train loss: 0.065\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[830] train loss: 0.065\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[840] train loss: 0.065\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[850] train loss: 0.065\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[860] train loss: 0.064\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[870] train loss: 0.064\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[880] train loss: 0.064\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[890] train loss: 0.272\n",
      "generated sentence:  contrary to popular [UNK] comes from a line in the [UNK] lorem ipsum comes from sections [UNK] making it look like readable [UNK] many desktop publishing packages and web page editors now use\n",
      "[900] train loss: 0.076\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[910] train loss: 0.125\n",
      "generated sentence:  contrary to popular [UNK] comes from a line in section [UNK] the standard chunk of lorem ipsum used since the 1500s is reproduced below for those [UNK] sections [UNK] sections [UNK] from [UNK]\n",
      "[920] train loss: 0.095\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[930] train loss: 0.061\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[940] train loss: 0.060\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[950] train loss: 0.060\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[960] train loss: 0.060\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[970] train loss: 0.061\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[980] train loss: 0.061\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[990] train loss: 0.061\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "[1000] train loss: 0.061\n",
      "generated sentence:  contrary to popular [UNK] lorem ipsum is not simply random [UNK] it has roots in a piece of classical latin literature from 45 [UNK] making it over 2000 years [UNK] richard [UNK] a\n",
      "Finished training.\n"
     ]
    }
   ],
   "source": [
    "text_encoder = TextEncoder(DATA_FILE_PATH)\n",
    "dataset = TextData(DATA_FILE_PATH, text_encoder, SEQUENCE_LEN)\n",
    "dataloader = DataLoader(dataset, BATCH_SIZE)\n",
    "vocab_size = text_encoder.vocab_size\n",
    "print(f\"Number of sequences in the dataset: {len(dataloader)}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Model Hyperparams\n",
    "model_args = dict(n_layer=2, \n",
    "                  n_head=2, \n",
    "                  n_embd=16, \n",
    "                  block_size=SEQUENCE_LEN, # sequence length\n",
    "                  bias=True, \n",
    "                  vocab_size=vocab_size, \n",
    "                  dropout=0.0) \n",
    "\n",
    "gpt_conf = GPTConfig(**model_args)\n",
    "model = GPT(gpt_conf).to(device)\n",
    "\n",
    "print_model_size(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), LEARNING_RATE)\n",
    "\n",
    "train_loop(model, dataloader, criterion, optimizer, EPOCHS, PROMPT, LOGGING_INTERVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a sentence and visualizing attention\n",
    "\n",
    "Now that we trained our model, we can play with it and generate sentences! (Again, keep in mind the model will only recognize words it has seen in the txt file you train on!). We can also visualize the attention.\n",
    "\n",
    "### What is Attention?\n",
    "In the context of transformer models like GPT, attention mechanisms are used to weigh the importance of different input tokens when predicting an output token. \n",
    "\n",
    "Visualizing attention weights can provide insights into the model's decision-making process. By exploring which tokens the model is focusing on when making a prediction, we can gain a better understanding of its learned patterns and potential areas of improvement.\n",
    "\n",
    "### How are we Visualizing Attention?\n",
    "In the following code, we extract the attention weights from our model and visualize them using a heatmap. The x-axis of the heatmap represents input tokens, while the color intensity represents the attention weight  darker colors indicate higher attention.\n",
    "\n",
    "<ul>\n",
    "    <li> Token Selection. We select a token of interest in a sentence and inspect how much attention it pays to other tokens when being predicted.\n",
    "    <li> Head Selection. Transformers contain multiple attention heads, each learning different patterns of attention. We select one of these heads for visualization.\n",
    "    <li> Heatmap. The heatmap represents attention weights from one token (e.g., \"gandalf\") towards all preceding tokens in the sentence.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lorem', 'ipsum', '[UNK]', 'but', 'the', 'majority', 'have', 'suffered', 'alteration', 'in', 'some', '[UNK]', 'by', 'injected', '[UNK]', 'or', 'randomised']\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"Lorem\"\n",
    "sentence, attn = generate_sentence(model, dataloader, PROMPT, 16, temperature=0.8, sample=True)\n",
    "sentence = sentence.split(\" \")\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now, attn_weights is of shape (1, model.n_head, T, T)\n",
    "# We get attention weights for our token of interest for the specified head\n",
    "token_idx = 6 #sentence.index('flying')\n",
    "head = 1\n",
    "\n",
    "attention = attn[0, head, token_idx].cpu().detach().numpy()\n",
    "attention = attention[:token_idx]\n",
    "\n",
    "# Convert the 1D vector to a 2D matrix and transpose for horizontal heatmap\n",
    "attention_matrix = attention.reshape(1, -1)\n",
    "\n",
    "# Now we'll plot the attention weights using a heatmap\n",
    "plt.subplots(figsize=(10, 4))\n",
    "sns.heatmap(attention_matrix, cmap=\"Blues\", annot=True, xticklabels=sentence[:token_idx], cbar_kws={'label': 'Attention Weight'})\n",
    "#sns.heatmap(attention_matrix, cmap=\"Blues\", cbar_kws={'label': 'Attention Weight'}, ax=ax, annot=True)\n",
    "\n",
    "\n",
    "plt.ylabel(f'Attention Weight from word x{sentence[token_idx]}')\n",
    "plt.xlabel('Tokens in Sentence')\n",
    "plt.title(f'Attention Weights of Head {head+1} for Token Index {token_idx}')\n",
    "plt.xticks(rotation=45)  # Rotate token names diagonally for better visibility\n",
    "plt.tight_layout()  # Ensures that everything fits without overlap\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "2567d625573dd00306b4578d011ae5968ef0ee87229e2f9d88073f62b5adab8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
